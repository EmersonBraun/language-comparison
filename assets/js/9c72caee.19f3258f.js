"use strict";(globalThis.webpackChunklanguage_comparison_docs=globalThis.webpackChunklanguage_comparison_docs||[]).push([[486],{9162(n,e,t){t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>i,default:()=>h,frontMatter:()=>c,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"concurrency-async","title":"Concurrency & Async","description":"Concurrency models -- async/await, threads, goroutines, and actors across 12 languages","source":"@site/docs/concurrency-async.md","sourceDirName":".","slug":"/concurrency-async","permalink":"/language-comparison/docs/concurrency-async","draft":false,"unlisted":false,"editUrl":"https://github.com/EmersonBraun/language-comparison/tree/main/docs/concurrency-async.md","tags":[],"version":"current","sidebarPosition":24,"frontMatter":{"sidebar_position":24,"description":"Concurrency models -- async/await, threads, goroutines, and actors across 12 languages","keywords":["concurrency","async","await","threads","goroutines","actors","parallel"]},"sidebar":"tutorialSidebar","previous":{"title":"Null Safety & Optionals","permalink":"/language-comparison/docs/null-safety"},"next":{"title":"Memory Management","permalink":"/language-comparison/docs/memory-management"}}');var r=t(4848),o=t(8453),s=t(6153);const c={sidebar_position:24,description:"Concurrency models -- async/await, threads, goroutines, and actors across 12 languages",keywords:["concurrency","async","await","threads","goroutines","actors","parallel"]},i="Concurrency & Async",l={},d=[{value:"Basic Async / Concurrent Execution",id:"basic-async--concurrent-execution",level:2},{value:"Thread Safety &amp; Synchronization",id:"thread-safety--synchronization",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function u(n){const e={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"concurrency--async",children:"Concurrency & Async"})}),"\n",(0,r.jsx)(e.p,{children:"Concurrency models are one of the biggest differentiators between programming languages. Here's how different languages handle asynchronous programming, threads, and concurrent execution."}),"\n","\n",(0,r.jsx)(e.h2,{id:"basic-async--concurrent-execution",children:"Basic Async / Concurrent Execution"}),"\n",(0,r.jsx)(s.A,{languages:[{name:"JavaScript",lang:"javascript",code:'// Promises\nconst fetchData = () => {\n  return new Promise((resolve) => {\n      setTimeout(() => resolve("data"), 1000);\n  });\n};\n\n// Async/await\nasync function main() {\n  const result = await fetchData();\n  console.log(result);\n}\n\n// Promise.all (parallel)\nconst [a, b] = await Promise.all([\n  fetchData(),\n  fetchData(),\n]);\n\n// Promise.race (first to complete)\nconst fastest = await Promise.race([\n  fetchData(),\n  fetchData(),\n]);'},{name:"PHP",lang:"php",code:'<?php\n// Fibers (PHP 8.1+)\n$fiber = new Fiber(function (): void {\n  $value = Fiber::suspend(\'fiber started\');\n  echo "Fiber resumed with: " . $value;\n});\n\n$result = $fiber->start();      // "fiber started"\n$fiber->resume(\'hello\');        // "Fiber resumed with: hello"\n\n// Parallel execution with pcntl_fork\n$pid = pcntl_fork();\nif ($pid == 0) {\n  // Child process\n  echo "Child process\\n";\n  exit(0);\n} else {\n  // Parent process\n  pcntl_wait($status);\n  echo "Parent process\\n";\n}\n\n// ReactPHP (event loop)\n// composer require react/event-loop\n$loop = React\\EventLoop\\Loop::get();\n$loop->addTimer(1.0, function () {\n  echo "Timer fired!\\n";\n});\n$loop->run();'},{name:"Rust",lang:"rust",code:'// Async/await with tokio\nuse tokio;\n\nasync fn fetch_data() -> String {\n  tokio::time::sleep(\n      tokio::time::Duration::from_secs(1)\n  ).await;\n  "data".to_string()\n}\n\n#[tokio::main]\nasync fn main() {\n  let result = fetch_data().await;\n  println!("{}", result);\n}\n\n// Parallel execution with join!\nlet (a, b) = tokio::join!(\n  fetch_data(),\n  fetch_data(),\n);\n\n// Spawning tasks\nlet handle = tokio::spawn(async {\n  fetch_data().await\n});\nlet result = handle.await.unwrap();'},{name:"Go",lang:"go",code:'// Goroutines (lightweight threads)\ngo func() {\n  fmt.Println("Hello from goroutine")\n}()\n\n// Channels for communication\nch := make(chan string)\n\ngo func() {\n  ch <- "hello"  // Send\n}()\n\nmsg := <-ch  // Receive\nfmt.Println(msg)\n\n// Select (multiplexing channels)\nselect {\ncase msg := <-ch1:\n  fmt.Println("From ch1:", msg)\ncase msg := <-ch2:\n  fmt.Println("From ch2:", msg)\ncase <-time.After(time.Second):\n  fmt.Println("Timeout")\n}\n\n// WaitGroup for synchronization\nvar wg sync.WaitGroup\nfor i := 0; i < 5; i++ {\n  wg.Add(1)\n  go func(id int) {\n      defer wg.Done()\n      fmt.Println("Worker", id)\n  }(i)\n}\nwg.Wait()'},{name:"Python",lang:"python",code:'import asyncio\n\n# Async/await\nasync def fetch_data():\n  await asyncio.sleep(1)\n  return "data"\n\nasync def main():\n  result = await fetch_data()\n  print(result)\n\nasyncio.run(main())\n\n# Parallel execution with gather\nasync def main():\n  a, b = await asyncio.gather(\n      fetch_data(),\n      fetch_data(),\n  )\n\n# Threading\nimport threading\n\ndef worker():\n  print("Worker thread")\n\nthread = threading.Thread(target=worker)\nthread.start()\nthread.join()\n\n# Multiprocessing\nfrom multiprocessing import Process\n\ndef worker():\n  print("Worker process")\n\np = Process(target=worker)\np.start()\np.join()'},{name:"Zig",lang:"zig",code:'const std = @import("std");\n\n// Threads\nfn worker(id: usize) void {\n  std.debug.print("Worker {}\\n", .{id});\n}\n\npub fn main() !void {\n  // Spawn threads\n  var threads: [5]std.Thread = undefined;\n  for (&threads, 0..) |*t, i| {\n      t.* = try std.Thread.spawn(.{}, worker, .{i});\n  }\n\n  // Join all threads\n  for (threads) |t| {\n      t.join();\n  }\n}\n\n// Async I/O with io_uring (Linux)\n// Zig provides low-level async through evented I/O\n// using std.event and std.io frameworks\n\n// Mutex for synchronization\nvar mutex = std.Thread.Mutex{};\nmutex.lock();\ndefer mutex.unlock();\n// critical section'},{name:"C#",lang:"csharp",code:'// Async/await\nasync Task<string> FetchDataAsync()\n{\n  await Task.Delay(1000);\n  return "data";\n}\n\nasync Task Main()\n{\n  string result = await FetchDataAsync();\n  Console.WriteLine(result);\n}\n\n// Parallel execution\nvar (a, b) = await (\n  FetchDataAsync(),\n  FetchDataAsync()\n);\n\n// Task.WhenAll\nvar results = await Task.WhenAll(\n  FetchDataAsync(),\n  FetchDataAsync()\n);\n\n// Parallel.ForEach\nParallel.ForEach(items, item =>\n{\n  Process(item);\n});\n\n// Threads\nvar thread = new Thread(() =>\n{\n  Console.WriteLine("Worker thread");\n});\nthread.Start();\nthread.Join();'},{name:"C++",lang:"cpp",code:'#include <thread>\n#include <future>\n#include <mutex>\n\n// Threads\nstd::thread t([]() {\n  std::cout << "Worker thread" << std::endl;\n});\nt.join();\n\n// Async/future\nauto future = std::async(std::launch::async, []() {\n  std::this_thread::sleep_for(\n      std::chrono::seconds(1)\n  );\n  return std::string("data");\n});\nstd::string result = future.get();\n\n// Mutex\nstd::mutex mtx;\n{\n  std::lock_guard<std::mutex> lock(mtx);\n  // critical section\n}\n\n// Condition variables\nstd::condition_variable cv;\nstd::unique_lock<std::mutex> lk(mtx);\ncv.wait(lk, []{ return ready; });'},{name:"C",lang:"c",code:'#include <pthread.h>\n\n// POSIX threads\nvoid* worker(void* arg) {\n  int id = *(int*)arg;\n  printf("Worker %d\\n", id);\n  return NULL;\n}\n\nint main() {\n  pthread_t threads[5];\n  int ids[5];\n\n  for (int i = 0; i < 5; i++) {\n      ids[i] = i;\n      pthread_create(&threads[i], NULL,\n                     worker, &ids[i]);\n  }\n\n  for (int i = 0; i < 5; i++) {\n      pthread_join(threads[i], NULL);\n  }\n\n  return 0;\n}\n\n// Mutex\npthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;\npthread_mutex_lock(&mutex);\n// critical section\npthread_mutex_unlock(&mutex);'},{name:"Java",lang:"java",code:'// CompletableFuture (async)\nCompletableFuture<String> future =\n  CompletableFuture.supplyAsync(() -> {\n      Thread.sleep(1000);\n      return "data";\n  });\n\nString result = future.get();\n\n// Parallel streams\nList<Integer> results = list.parallelStream()\n  .map(x -> x * 2)\n  .collect(Collectors.toList());\n\n// Virtual threads (Java 21+)\nThread.startVirtualThread(() -> {\n  System.out.println("Virtual thread");\n});\n\n// ExecutorService\nExecutorService executor =\n  Executors.newFixedThreadPool(4);\nFuture<String> f = executor.submit(() -> "result");\nexecutor.shutdown();\n\n// Threads\nThread thread = new Thread(() -> {\n  System.out.println("Worker");\n});\nthread.start();\nthread.join();'},{name:"Ruby",lang:"ruby",code:'# Threads\nthread = Thread.new do\nputs "Worker thread"\nend\nthread.join\n\n# Multiple threads\nthreads = 5.times.map do |i|\nThread.new(i) do |id|\n  puts "Worker #{id}"\nend\nend\nthreads.each(&:join)\n\n# Mutex\nmutex = Mutex.new\nmutex.synchronize do\n# critical section\nend\n\n# Fibers (cooperative concurrency)\nfiber = Fiber.new do\nFiber.yield "first"\nFiber.yield "second"\n"third"\nend\n\nputs fiber.resume  # "first"\nputs fiber.resume  # "second"\n\n# Ractor (Ruby 3.0+ true parallelism)\nractor = Ractor.new do\nRactor.yield "hello from ractor"\nend\nputs ractor.take'},{name:"Swift",lang:"swift",code:'// Structured concurrency (Swift 5.5+)\nfunc fetchData() async -> String {\n  try? await Task.sleep(nanoseconds: 1_000_000_000)\n  return "data"\n}\n\n// Async/await\nTask {\n  let result = await fetchData()\n  print(result)\n}\n\n// Parallel execution with async let\nasync let a = fetchData()\nasync let b = fetchData()\nlet results = await (a, b)\n\n// TaskGroup\nawait withTaskGroup(of: String.self) { group in\n  for i in 0..<5 {\n      group.addTask {\n          return await fetchData()\n      }\n  }\n  for await result in group {\n      print(result)\n  }\n}\n\n// Actors (thread-safe state)\nactor Counter {\n  private var count = 0\n  func increment() { count += 1 }\n  func getCount() -> Int { count }\n}'}]}),"\n",(0,r.jsx)(e.h2,{id:"thread-safety--synchronization",children:"Thread Safety & Synchronization"}),"\n",(0,r.jsx)(s.A,{languages:[{name:"JavaScript",lang:"javascript",code:"// JavaScript is single-threaded (event loop)\n// No shared memory issues in main thread\n\n// Web Workers (browser) / Worker Threads (Node.js)\nconst { Worker } = require('worker_threads');\n\nconst worker = new Worker('./worker.js');\nworker.postMessage({ data: 'hello' });\nworker.on('message', (msg) => {\n  console.log('From worker:', msg);\n});\n\n// SharedArrayBuffer (shared memory)\nconst buffer = new SharedArrayBuffer(1024);\nconst view = new Int32Array(buffer);\nAtomics.add(view, 0, 1);  // Atomic operation"},{name:"PHP",lang:"php",code:'<?php\n// PHP is typically single-threaded per request\n// Use extensions for true threading\n\n// Shared memory with shmop\n$shm = shmop_open(123, "c", 0644, 100);\nshmop_write($shm, "data", 0);\n$data = shmop_read($shm, 0, 4);\nshmop_delete($shm);\n\n// Semaphores\n$sem = sem_get(1234);\nsem_acquire($sem);\n// critical section\nsem_release($sem);'},{name:"Rust",lang:"rust",code:'use std::sync::{Arc, Mutex, RwLock};\n\n// Mutex (mutual exclusion)\nlet counter = Arc::new(Mutex::new(0));\n\nlet handles: Vec<_> = (0..5).map(|_| {\n  let counter = Arc::clone(&counter);\n  std::thread::spawn(move || {\n      let mut num = counter.lock().unwrap();\n      *num += 1;\n  })\n}).collect();\n\nfor handle in handles {\n  handle.join().unwrap();\n}\n\n// RwLock (multiple readers, one writer)\nlet data = Arc::new(RwLock::new(vec![1, 2, 3]));\nlet read = data.read().unwrap();\n// Channels\nuse std::sync::mpsc;\nlet (tx, rx) = mpsc::channel();\ntx.send("hello").unwrap();\nlet msg = rx.recv().unwrap();'},{name:"Go",lang:"go",code:'import "sync"\n\n// Mutex\nvar mu sync.Mutex\nmu.Lock()\n// critical section\nmu.Unlock()\n\n// RWMutex\nvar rw sync.RWMutex\nrw.RLock()   // Read lock\nrw.RUnlock()\nrw.Lock()    // Write lock\nrw.Unlock()\n\n// Atomic operations\nimport "sync/atomic"\nvar counter int64\natomic.AddInt64(&counter, 1)\nval := atomic.LoadInt64(&counter)\n\n// Once (run exactly once)\nvar once sync.Once\nonce.Do(func() {\n  fmt.Println("Only once")\n})'},{name:"Python",lang:"python",code:'import threading\n\n# Lock\nlock = threading.Lock()\nwith lock:\n  # critical section\n  pass\n\n# RLock (reentrant)\nrlock = threading.RLock()\n\n# Semaphore\nsem = threading.Semaphore(3)\nwith sem:\n  # max 3 concurrent\n  pass\n\n# Event\nevent = threading.Event()\nevent.set()    # Signal\nevent.wait()   # Wait for signal\nevent.clear()  # Reset\n\n# Queue (thread-safe)\nfrom queue import Queue\nq = Queue()\nq.put("item")\nitem = q.get()'},{name:"Zig",lang:"zig",code:'const std = @import("std");\n\n// Mutex\nvar mutex = std.Thread.Mutex{};\nmutex.lock();\ndefer mutex.unlock();\n// critical section\n\n// Atomic operations\nvar counter = std.atomic.Value(u32).init(0);\n_ = counter.fetchAdd(1, .seq_cst);\nconst val = counter.load(.seq_cst);\n\n// ResetEvent (signaling)\nvar event = std.Thread.ResetEvent{};\nevent.set();   // Signal\nevent.wait();  // Wait'},{name:"C#",lang:"csharp",code:'// Lock\nprivate readonly object _lock = new();\n\nlock (_lock)\n{\n  // critical section\n}\n\n// SemaphoreSlim\nvar sem = new SemaphoreSlim(3);\nawait sem.WaitAsync();\ntry { /* work */ }\nfinally { sem.Release(); }\n\n// Channel (producer-consumer)\nvar channel = Channel.CreateUnbounded<string>();\nawait channel.Writer.WriteAsync("item");\nvar item = await channel.Reader.ReadAsync();\n\n// Interlocked (atomic)\nint counter = 0;\nInterlocked.Increment(ref counter);'},{name:"C++",lang:"cpp",code:"#include <mutex>\n#include <atomic>\n#include <shared_mutex>\n\n// Mutex with lock_guard\nstd::mutex mtx;\n{\n  std::lock_guard<std::mutex> lock(mtx);\n  // critical section\n}\n\n// shared_mutex (read-write lock)\nstd::shared_mutex rw_mtx;\n{\n  std::shared_lock lock(rw_mtx);  // Read\n}\n{\n  std::unique_lock lock(rw_mtx);  // Write\n}\n\n// Atomic\nstd::atomic<int> counter{0};\ncounter.fetch_add(1);\nint val = counter.load();"},{name:"C",lang:"c",code:"#include <pthread.h>\n#include <stdatomic.h>\n\n// Mutex\npthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;\npthread_mutex_lock(&mutex);\n// critical section\npthread_mutex_unlock(&mutex);\n\n// Read-write lock\npthread_rwlock_t rwlock = PTHREAD_RWLOCK_INITIALIZER;\npthread_rwlock_rdlock(&rwlock);  // Read\npthread_rwlock_unlock(&rwlock);\npthread_rwlock_wrlock(&rwlock);  // Write\npthread_rwlock_unlock(&rwlock);\n\n// Atomic (C11)\natomic_int counter = 0;\natomic_fetch_add(&counter, 1);\nint val = atomic_load(&counter);"},{name:"Java",lang:"java",code:'// synchronized\nsynchronized (this) {\n  // critical section\n}\n\n// ReentrantLock\nReentrantLock lock = new ReentrantLock();\nlock.lock();\ntry { /* work */ }\nfinally { lock.unlock(); }\n\n// AtomicInteger\nAtomicInteger counter = new AtomicInteger(0);\ncounter.incrementAndGet();\nint val = counter.get();\n\n// ConcurrentHashMap\nConcurrentHashMap<String, Integer> map =\n  new ConcurrentHashMap<>();\nmap.put("key", 1);\n\n// BlockingQueue\nBlockingQueue<String> queue =\n  new LinkedBlockingQueue<>();\nqueue.put("item");\nString item = queue.take();'},{name:"Ruby",lang:"ruby",code:"# Mutex\nmutex = Mutex.new\nmutex.synchronize do\n# critical section\nend\n\n# Queue (thread-safe)\nrequire 'thread'\nqueue = Queue.new\nqueue << \"item\"\nitem = queue.pop\n\n# ConditionVariable\ncv = ConditionVariable.new\nmutex.synchronize do\ncv.wait(mutex)  # Wait\nend\nmutex.synchronize do\ncv.signal        # Wake one\ncv.broadcast     # Wake all\nend\n\n# Concurrent::Ruby gem\n# gem install concurrent-ruby\nrequire 'concurrent'\nfuture = Concurrent::Future.execute { 42 }\nputs future.value"},{name:"Swift",lang:"swift",code:"// Actors (built-in thread safety)\nactor BankAccount {\n  private var balance: Double = 0\n\n  func deposit(_ amount: Double) {\n      balance += amount\n  }\n\n  func getBalance() -> Double {\n      return balance\n  }\n}\n\nlet account = BankAccount()\nawait account.deposit(100)\nlet balance = await account.getBalance()\n\n// DispatchQueue\nDispatchQueue.global().async {\n  // Background work\n  DispatchQueue.main.async {\n      // Update UI\n  }\n}\n\n// NSLock\nlet lock = NSLock()\nlock.lock()\n// critical section\nlock.unlock()"}]}),"\n",(0,r.jsx)(e.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Concurrency models vary widely"})," -- Go uses goroutines with channels (CSP style): ",(0,r.jsx)(e.code,{children:'go func() { ch <- "hello" }'})," and ",(0,r.jsx)(e.code,{children:"msg := <-ch"}),". JavaScript, Python, Rust, C#, and Swift use async/await: ",(0,r.jsx)(e.code,{children:"async def fetch_data()"})," in Python, ",(0,r.jsx)(e.code,{children:"async fn fetch_data()"})," in Rust with tokio, ",(0,r.jsx)(e.code,{children:"async Task<string> FetchDataAsync()"})," in C#. C, C++, and Java use traditional OS threads with ",(0,r.jsx)(e.code,{children:"pthread_create"}),", ",(0,r.jsx)(e.code,{children:"std::thread"}),", or ",(0,r.jsx)(e.code,{children:"Thread.start()"}),", plus mutexes for synchronization. Goroutines are lightweight (millions possible); threads are heavier. Async/await suits I/O-bound work; threads suit CPU-bound parallelism. Choose Go for simple concurrency; choose async/await for I/O-heavy servers; choose threads when you need true parallelism and control."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Channel-based communication is first-class in Go"})," -- Go's built-in channels and ",(0,r.jsx)(e.code,{children:"select"})," make producer-consumer patterns idiomatic: ",(0,r.jsx)(e.code,{children:"ch := make(chan string)"}),", ",(0,r.jsx)(e.code,{children:'ch <- "msg"'}),", ",(0,r.jsx)(e.code,{children:"msg := <-ch"}),", and ",(0,r.jsx)(e.code,{children:"select { case msg := <-ch1: ... case msg := <-ch2: ... }"}),". Rust offers ",(0,r.jsx)(e.code,{children:"std::sync::mpsc::channel()"})," and C# has ",(0,r.jsx)(e.code,{children:"Channel.CreateUnbounded<T>()"}),", but they are library features. Go's design encourages \"share memory by communicating\" rather than mutexes. If you build pipelines, worker pools, or event-driven systems, Go's channels are the most ergonomic; Rust and C# provide similar power with more setup."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Go and Rust excel at concurrency"})," -- Go's goroutines are lightweight (2KB stacks, multiplexed onto OS threads) and easy to spawn: ",(0,r.jsx)(e.code,{children:"go worker(id)"}),". Rust's ownership model prevents data races at compile time: the compiler rejects sharing mutable state across threads without synchronization. For example, ",(0,r.jsx)(e.code,{children:"Arc<Mutex<i32>>"})," in Rust ensures safe shared access. Go relies on channels and discipline; Rust enforces safety statically. Choose Go when you want simple, fast concurrency with minimal friction; choose Rust when you need maximum safety and performance without a garbage collector."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Swift introduces actors"})," -- Actors provide built-in thread-safe shared state by serializing access: ",(0,r.jsx)(e.code,{children:"actor Counter { private var count = 0; func increment() { count += 1 } }"})," -- all methods run serially, so no data races. You call with ",(0,r.jsx)(e.code,{children:"await account.deposit(100)"}),". This replaces manual ",(0,r.jsx)(e.code,{children:"Mutex"}),"/",(0,r.jsx)(e.code,{children:"Lock"})," patterns. Swift's actors integrate with async/await and structured concurrency. If you build concurrent systems in Swift (e.g., iOS/macOS), prefer actors over manual locking; for other languages, use mutexes or channel-based designs."]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"JavaScript is single-threaded"})," -- The event loop handles async via callbacks and Promises: ",(0,r.jsx)(e.code,{children:"await fetch('/api')"})," does not block the thread; it yields until the response arrives. True parallelism requires Web Workers (browser) or Worker Threads (Node.js) with ",(0,r.jsx)(e.code,{children:"postMessage"})," for communication. CPU-heavy work (hashing, image processing) blocks the main thread; offload it to workers. For I/O-bound Node.js servers, async/await is sufficient; for CPU-bound tasks, add workers or consider a multi-threaded language like Go or Rust."]}),"\n"]}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(u,{...n})}):u(n)}}}]);